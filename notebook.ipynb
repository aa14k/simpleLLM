{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f9548f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self,ff_dim: int,embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.lin1,self.lin2 = nn.Linear(embed_dim,ff_dim),nn.Linear(ff_dim,embed_dim)\n",
    "    def forward(self,x):\n",
    "        return self.lin2(F.gelu(self.lin1(x)))\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self,embed_dim: int, n_experts: int):\n",
    "        super().__init__()\n",
    "        self.lin1= nn.Linear(embed_dim,n_experts)\n",
    "    def forward(self,x):\n",
    "        return F.softmax(self.lin1(x),dim=-1)\n",
    "    \n",
    "class MoE(nn.Module):\n",
    "    def __init__(self,ff_dim: int,embed_dim: int, n_experts: int, k: int):\n",
    "        super().__init__()\n",
    "        self.router = Router(embed_dim,n_experts)\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(ff_dim,embed_dim)\n",
    "            for _ in range(n_experts)\n",
    "        ])\n",
    "\n",
    "        self.k = k\n",
    "    \n",
    "    def forward(self,x_ble, k=None):\n",
    "        #step 1: get scores\n",
    "        scores_bln = self.router(x_ble)\n",
    "        #step 2: select top-k\n",
    "        K = self.k\n",
    "        if k is not None: K=k\n",
    "        vals_blk,idxs_blk = torch.topk(scores_bln,K)\n",
    "        vals_blk = F.normalize(vals_blk,p=1,dim=-1)\n",
    "\n",
    "        #step 3+4: compute and weight\n",
    "        out_ble = torch.zeros_like(x_ble)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            batch_idx,seq_idx,k_idx = torch.where(idxs_blk == i)\n",
    "            out_ble[batch_idx,seq_idx] += expert(x_ble[batch_idx,seq_idx]) * vals_blk[batch_idx,seq_idx,k_idx][:,None]\n",
    "        return out_ble \n",
    "\n",
    "\n",
    "class MoE1(nn.Module):\n",
    "    def __init__(self, ff_dim: int, embed_dim: int, n_experts: int, k: int):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.router = Router(embed_dim, n_experts)\n",
    "        \n",
    "        # TODO: Define w1 and w2 as nn.Parameters with the shapes we agreed on\n",
    "        stdv = 1. / math.sqrt(embed_dim)\n",
    "        self.w1_nfe = nn.Parameter(2 * stdv * torch.rand(n_experts,ff_dim,embed_dim) - stdv) \n",
    "        stdv = 1. / math.sqrt(ff_dim)\n",
    "        self.w2_nef = nn.Parameter(2 * stdv * torch.rand(n_experts,embed_dim,ff_dim) - stdv) \n",
    "\n",
    "    def forward(self, x_ble):\n",
    "        scores_bln = self.router(x_ble)\n",
    "        vals_blk,idxs_blk = torch.topk(scores_bln,self.k,sorted=False)\n",
    "        mask_bln = torch.zeros_like(scores_bln).scatter(2,idxs_blk,1)\n",
    "        scores_bln = F.normalize(scores_bln * mask_bln,p=1,dim=-1)\n",
    "\n",
    "        out_blnf = torch.einsum('ble,nfe->blnf',x_ble,self.w1_nfe)\n",
    "        out_blnf = F.gelu(out_blnf)\n",
    "        out_blne = torch.einsum('blnf,nef->blne',out_blnf,self.w2_nef)\n",
    "        out_ble = torch.einsum('blne,bln->ble',out_blne,scores_bln)\n",
    "        return out_ble\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b9ed2bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 64, 8])\n",
      "Output shape: torch.Size([32, 64, 8])\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup constants\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 64\n",
    "EMBED_DIM = 8\n",
    "FF_DIM = 32\n",
    "N_EXPERTS = 4\n",
    "K = 2 # Top-2 experts\n",
    "\n",
    "# 2. Create dummy input\n",
    "# Shape: [Batch, Seq, Embed]\n",
    "x = torch.randn(BATCH_SIZE, SEQ_LEN, EMBED_DIM)\n",
    "\n",
    "# 3. Initialize Model\n",
    "model = MoE1(FF_DIM, EMBED_DIM, N_EXPERTS, K)\n",
    "\n",
    "# 4. Run Forward Pass\n",
    "# This will return zeros right now because the loop is empty!\n",
    "output = model(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "25f7e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is output non-zero? True\n"
     ]
    }
   ],
   "source": [
    "# Check if output is non-zero\n",
    "print(\"Is output non-zero?\", torch.any(output != 0).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A,B = torch.rand(5,5),torch.rand(5,5)\n",
    "C = torch.zeros_like(A)\n",
    "\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(B.shape[1]):\n",
    "        total = 0.0\n",
    "        for k in range(A.shape[0]):\n",
    "            total += A[i,k] * B[k,j]\n",
    "        C[i,j] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "40b8c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.einsum('ik,kj->ij',A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d48df0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "A,B = torch.rand(5),torch.rand(5)\n",
    "C = torch.einsum('i,i->',A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e8c91229",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch,length,embed,ff,n_exp = 32,64,16,32,8\n",
    "A_ble,B_nfe,C_nef = torch.rand(batch,length,embed),torch.rand(n_exp,ff,embed), torch.rand(n_exp,embed,ff)\n",
    "probs_bln = torch.rand(batch,length,n_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a3248b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_blnf = torch.einsum('ble,nfe->blnf',A_ble,B_nfe)\n",
    "out_blnf = F.gelu(out_blnf)\n",
    "out_blne = torch.einsum('blnf,nef->blne',out_blnf,C_nef)\n",
    "out_ble = torch.einsum('blne,bln->ble',out_blne,probs_bln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "56c4b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(0,2,(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "96870d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 4])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bincount(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d517cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_experts: int):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embed_dim, n_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.lin1(x), dim=-1)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, ff_dim: int, embed_dim: int, n_experts: int, k: int, capacity: int = 512):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.n_experts = n_experts\n",
    "        self.router = Router(embed_dim, n_experts)\n",
    "        \n",
    "        # Dense parameters for all experts (Vectorized)\n",
    "        stdv = 1. / math.sqrt(embed_dim)\n",
    "        self.w1_nfe = nn.Parameter(2 * stdv * torch.rand(n_experts, ff_dim, embed_dim) - stdv) \n",
    "        stdv = 1. / math.sqrt(ff_dim)\n",
    "        self.w2_nef = nn.Parameter(2 * stdv * torch.rand(n_experts, embed_dim, ff_dim) - stdv) \n",
    "\n",
    "    def forward(self, x_ble):\n",
    "        # --- Step 1: Routing ---\n",
    "        scores_bln = self.router(x_ble)\n",
    "        vals_blk, idxs_blk = torch.topk(scores_bln, self.k, sorted=False)\n",
    "        vals_blk = F.normalize(vals_blk, p=1, dim=-1)\n",
    "\n",
    "        # --- Step 2: Flattening ---\n",
    "        # Treat batch+seq as one long list of tokens\n",
    "        x_te = x_ble.flatten(0, 1) #t=b*l\n",
    "        idxs_r = idxs_blk.flatten() # All expert choices (r=b*l*k)\n",
    "\n",
    "        # --- Step 3: Permutation / Sorting ---\n",
    "        # 3a. Create Source Indices (who sent this request?)\n",
    "        src_r = torch.repeat_interleave(torch.arange(x_te.shape[0], device=x_ble.device), K)\n",
    "        \n",
    "        # 3b. Sort by Expert ID to group requests together\n",
    "        perm_r = torch.argsort(idxs_r)\n",
    "        \n",
    "        # 3c. Apply the Sort\n",
    "        idxs_r = idxs_r[perm_r]   # Sorted expert IDs: [0, 0, ..., 1, 1, ...]\n",
    "        src_r = src_r[perm_r]     # Corresponding source tokens\n",
    "\n",
    "        # --- Step 4: Grouping & Capacity (Current Step) ---\n",
    "        # Calculate how many tokens each expert has\n",
    "        counts_n = torch.bincount(idxs_r, minlength=self.n_experts)\n",
    "        \n",
    "        # Calculate where each expert's block starts in the sorted list\n",
    "        # cumsum gives the ends: [3, 5, ...]\n",
    "        cumulative_n = torch.cumsum(counts_n, dim=0)\n",
    "        \n",
    "        # We prepend 0 to get the starts: [0, 3, 5, ...]\n",
    "        starts_nplus1 = torch.cat([torch.tensor([0], device=x_ble.device), cumulative_n[:-1]])\n",
    "        \n",
    "        # ... We are here ...\n",
    "\n",
    "        ranks_r = torch.arange(idxs_r.shape[0]) - starts_nplus1[idxs_r]\n",
    "\n",
    "        mask = (ranks_r < self.capacity)\n",
    "        idxs_leqc = idxs_r[mask]\n",
    "        src_leqc = src_r[mask]\n",
    "        ranks_leqc = ranks_r[mask]\n",
    "\n",
    "\n",
    "        return x_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "351cc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [True,False,False]\n",
    "\n",
    "x = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "eb55bcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad470127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
