{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9548f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def casual_attention_mask(seq_len):\n",
    "    return torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 rotary,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.rotary = rotary\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        for lin in (self.q_proj, self.k_proj, self.v_proj, self.out_proj):\n",
    "            nn.init.xavier_uniform_(lin.weight)\n",
    "            nn.init.zeros_(lin.bias)\n",
    "        \n",
    "    def forward(self, inputs, past_kv=None, use_cache: bool = False):\n",
    "        B, L, D = inputs.shape\n",
    "\n",
    "        # 1) project\n",
    "        q = self.q_proj(inputs).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(inputs).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(inputs).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 2) past length\n",
    "        past_len = 0\n",
    "        if past_kv is not None:\n",
    "            # TODO: set past_len from past_kv[0]\n",
    "            pass\n",
    "\n",
    "        # 3) RoPE with offset\n",
    "        # TODO: call self.rotary.rotation(q, k, start_pos=past_len)\n",
    "        # q, k = ...\n",
    "\n",
    "        # 4) append cache\n",
    "        if past_kv is not None:\n",
    "            past_k, past_v = past_kv\n",
    "            # TODO: concat along sequence dim (dim=2)\n",
    "            # k = ...\n",
    "            # v = ...\n",
    "\n",
    "        # 5) mask logic\n",
    "        # Keep your exact old behavior for the no-cache full-seq case.\n",
    "        # For cached incremental (typical L==1), you can use mask=None.\n",
    "        mask = None\n",
    "        if past_kv is None:\n",
    "            # TODO: old mask path (only when L>1)\n",
    "            pass\n",
    "        else:\n",
    "            # mask=None is fine for L==1 cached decode\n",
    "            mask = None\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0, attn_mask=mask)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        y = self.out_proj(y)\n",
    "\n",
    "        if use_cache:\n",
    "            return y, (k, v)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df63e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
