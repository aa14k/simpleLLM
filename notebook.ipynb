{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eec897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def casual_attention_mask(seq_len):\n",
    "    return torch.triu(torch.ones(seq_len,seq_len,dtype=torch.bool), diagonal=1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    #for proper use of muon\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "\n",
    "        for lin in (self.q_proj, self.k_proj, self.v_proj, self.out_proj):\n",
    "            nn.init.xavier_uniform_(lin.weight)\n",
    "            nn.init.zeros_(lin.bias)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        B,L,D = inputs.shape\n",
    "\n",
    "        mask = ~casual_attention_mask(L).to(inputs.device)\n",
    "\n",
    "        q = self.q_proj(inputs).view(B,L,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        k = self.k_proj(inputs).view(B,L,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        v = self.v_proj(inputs).view(B,L,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q,k,v,\n",
    "                                           dropout_p=0.0,\n",
    "                                           attn_mask=mask)\n",
    "        \n",
    "        y = y.transpose(1,2).contiguous().view(B,L,D)\n",
    "        return self.out_proj(y)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 ff_dim: int,\n",
    "                 RoPE,\n",
    "                 *,\n",
    "                 dropout_rate: float = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Match JAX: no dropout inside attention weights\n",
    "        self.mha = MultiHeadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            rotary=RoPE,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        nn.init.ones_(self.ln1.weight)\n",
    "        nn.init.zeros_(self.ln1.bias)\n",
    "\n",
    "        self.lin1 = nn.Linear(embed_dim, ff_dim)\n",
    "        nn.init.xavier_uniform_(self.lin1.weight)\n",
    "        nn.init.zeros_(self.lin1.bias)\n",
    "\n",
    "        self.lin2 = nn.Linear(ff_dim, embed_dim)\n",
    "        nn.init.xavier_uniform_(self.lin2.weight)\n",
    "        nn.init.zeros_(self.lin2.bias)\n",
    "\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        nn.init.ones_(self.ln2.weight)\n",
    "        nn.init.zeros_(self.ln2.bias)\n",
    "\n",
    "    def forward(self, inputs, training: bool = False):\n",
    "        attention_output = self.mha(inputs)\n",
    "\n",
    "        # Respect the explicit training flag (like JAX deterministic=not training)\n",
    "        attention_output = F.dropout(attention_output, p=self.dropout_rate, training=training)\n",
    "        out1 = self.ln1(inputs + attention_output)\n",
    "\n",
    "        ffn_output = self.lin1(out1)\n",
    "        ffn_output = F.relu(ffn_output)\n",
    "        ffn_output = self.lin2(ffn_output)\n",
    "        ffn_output = F.dropout(ffn_output, p=self.dropout_rate, training=training)\n",
    "\n",
    "        return self.ln2(out1 + ffn_output)\n",
    "\n",
    "class ActionandRoPEEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 maxlen: int,\n",
    "                 vocab_size: int,\n",
    "                 embed_dim: int,\n",
    "                 base: float = 10000.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.base = base\n",
    "        \n",
    "        self.action_emb = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = embed_dim,\n",
    "        )\n",
    "\n",
    "        self.frequencies()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return  self.action_emb(x)\n",
    "\n",
    "    def frequencies(self):\n",
    "        m = torch.arange(self.maxlen,device='cuda')\n",
    "        theta = torch.pow(self.base,-2.0*torch.arange(self.embed_dim//2,device='cuda')/self.embed_dim)\n",
    "        freqs = torch.outer(m,theta)\n",
    "        self.cos, self.sin = torch.cos(freqs), torch.sin(freqs)\n",
    "    \n",
    "    def rotation(self,q,k):\n",
    "\n",
    "        #reshaping the input (Batch, Head, Len, D) #please use english here\n",
    "        qr,qi = q.reshape(q.shape[:-1] + (-1,2)).unbind(-1)\n",
    "        kr,ki = k.reshape(k.shape[:-1] + (-1,2)).unbind(-1)\n",
    "\n",
    "        qr = qr * self.cos - qi * self.sin\n",
    "        qi = qr * self.sin + qi * self.cos\n",
    "        kr = kr * self.cos - ki * self.sin\n",
    "        ki = kr * self.sin + ki * self.cos\n",
    "\n",
    "        qout = torch.stack([qr,qi],dim=-1).flatten(3)\n",
    "        kout = torch.stack([kr,ki],dim=-1).flatten(3)\n",
    "\n",
    "        return qout,kout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 maxlen: int,\n",
    "                 vocab_size: int,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 feed_forward_dim: int,\n",
    "                 num_transformer_blocks: int,\n",
    "                 tokenizer,\n",
    "                 top_k: int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "        self.top_k = top_k\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.embedding_layer = ActionandRoPEEmbedding(maxlen, vocab_size, embed_dim)\n",
    "\n",
    "        # ModuleList so we can pass training=... like JAX does\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, feed_forward_dim, self.embedding_layer)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "        # Cache end token id (same as your JAX code)\n",
    "        self.end_token_id = self.tokenizer.encode(\n",
    "            \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "    def forward(self, inputs, training: bool = False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def sample_from(self, logits, generator: torch.Generator | None = None):\n",
    "        # logits: (vocab_size,) or (..., vocab_size)\n",
    "        k = min(self.top_k, logits.size(-1))\n",
    "        topk_logits, topk_indices = torch.topk(logits, k=k, dim=-1)\n",
    "\n",
    "        probs = F.softmax(topk_logits, dim=-1)\n",
    "\n",
    "        # sample an index in [0, k)\n",
    "        sampled_in_topk = torch.multinomial(probs, num_samples=1, generator=generator)\n",
    "\n",
    "        # map back to vocab ids\n",
    "        next_token = topk_indices.gather(-1, sampled_in_topk).squeeze(-1)\n",
    "        return next_token\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_step(self, padded_tokens, sample_index: int, generator: torch.Generator | None = None):\n",
    "        logits = self.forward(padded_tokens, training=False)          # (1, L, vocab)\n",
    "        return self.sample_from(logits[0, sample_index], generator)   # (vocab,) -> token id\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_text(self, max_tokens: int, start_tokens: list[int], pad_token_id: int = 0, seed: int | None = None):\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        generator = None\n",
    "        if seed is not None:\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "        generated: list[int] = []\n",
    "        print(self.tokenizer.decode(start_tokens), flush=True, end=\"\")\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "\n",
    "            tokens = start_tokens + generated\n",
    "\n",
    "            # Optional safety (JAX code will break if you exceed maxlen)\n",
    "            if len(tokens) > self.maxlen:\n",
    "                tokens = tokens[-self.maxlen:]\n",
    "                sample_index = self.maxlen - 1\n",
    "\n",
    "            padded = tokens + [pad_token_id] * (self.maxlen - len(tokens))\n",
    "            padded_tokens = torch.tensor(padded, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index, generator=generator))\n",
    "            if next_token == self.end_token_id:\n",
    "                break\n",
    "\n",
    "            generated.append(next_token)\n",
    "            print(self.tokenizer.decode([next_token]), flush=True, end=\"\")\n",
    "\n",
    "        return self.tokenizer.decode(start_tokens + generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d94cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
